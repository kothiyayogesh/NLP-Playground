{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have you ever wondered how your email filters out spam messages? If yes, then you are about to find out how and implement the solution by yourselves. It is primarily text classification problem.\n",
    "\n",
    "<img src=\"../../Images/spam_filtering.png\" alt=\"Drawing\" style=\"width: 150px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of content:\n",
    "-  Introduction\n",
    "-  Import library\n",
    "-  Load dataset\n",
    "-  Explore the dataset\n",
    "-  Pre-processing the text data\n",
    "-  Feature Engineering\n",
    "  - Feature Creation\n",
    "  - Feature evaluation\n",
    "-  Split into train/test\n",
    "-  Text to Features\n",
    "  - Bag of words\n",
    "  - TF-IDF\n",
    "  - Word embedding\n",
    "-  Machine Learning Classifiers\n",
    "  - Random Forest model\n",
    "-  Cross - validation\n",
    "-  Grid-search\n",
    "-  Model selection\n",
    "-  Evaluation metric\n",
    "-  Final step\n",
    "-  Summary \n",
    "-  Credits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this tutorial, we'll cover some basics of natural language processing like reading in and creating structure in messy text data, and then cleaning and tokenizing that data. Then we'll cover some of the more advanced topics like lemmatizing and vectorizing your data. In other words, converting it from text into a numeric matrix. We'll do this with a focus on preparing our data to build a machine learning classifier on top of it. We'll learn how to build machine learning classifier, while thoroughly testing and evaluating different variations of the model. You'll have the tools to go from messy dataset to concise and accurate predictions from machine learning model, to deliver solutions to complex business problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python provides developers with extensive libraries that handle many NLP-related tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "fullCorpus = pd.read_csv(\"spam.csv\", encoding='latin')\n",
    "fullCorpus.dropna(axis=1, inplace=True)\n",
    "fullCorpus.columns = ['label', 'body_text']\n",
    "\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the well curated data sets can have human errors, and it's vital that we spot quirks with the data and fix/remove them before investing too much time in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data has 5572 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "# What is the shape of the dataset?\n",
    "print(\"Input data has {} rows and {} columns\".format(len(fullCorpus), len(fullCorpus.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out of 5572 rows, 747 are spam, 4825 are ham\n"
     ]
    }
   ],
   "source": [
    "# How many spam/ham are there?\n",
    "print(\"Out of {} rows, {} are spam, {} are ham\".format(len(fullCorpus),\n",
    "                                                       len(fullCorpus[fullCorpus['label']=='spam']),\n",
    "                                                       len(fullCorpus[fullCorpus['label']=='ham'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of null in label: 0\n",
      "Number of null in text: 0\n"
     ]
    }
   ],
   "source": [
    "# How much missing data is there?\n",
    "print(\"Number of null in label: {}\".format(fullCorpus['label'].isnull().sum()))\n",
    "print(\"Number of null in text: {}\".format(fullCorpus['body_text'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text can come in a variety of forms from a list of individual words, to sentences to multiple paragraphs with special characters (like tweets for example). This step is very important in pipeline which helps to feed right data to the model. Generally this steps involve **cleaning**, **lowercasing**, **stemming/lemmatization**, **stop-word removal**, **normalization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pattern = re.compile(r'\\s+')\n",
    "lemma = WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Normalization\n",
    "## Contraction dictionary is stored in JSON file as Python dictionary\n",
    "contractions_data=open(\"../../utils/contractions.json\", encoding=\"utf8\").read()\n",
    "contractions = json.loads(contractions_data)\n",
    "\n",
    "p_space = re.compile(r'[^\\x20-\\x7e]')\n",
    "\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\",\n",
    "                 \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '”': '\"', '“': '\"', \"£\": \"e\",\n",
    "                 '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta',\n",
    "                 '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "\n",
    "puncts = list(string.punctuation)\n",
    "puncts.extend([',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£',\n",
    "    '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…',\n",
    "    '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─',\n",
    "    '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞',\n",
    "    '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ' '])\n",
    "\n",
    "puncts = \"\".join(puncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(doc):\n",
    "\n",
    "    doc = doc.lower()                                          ## Lower casing\n",
    "    \n",
    "    # Clean invisible chars\n",
    "    doc = p_space.sub(r'', doc)                                ## Cleaning - Invisible character\n",
    "\n",
    "    ## Expanding contraction\n",
    "    for key, value in contractions.items() :                   ## Normalization\n",
    "        if key in doc :\n",
    "            doc = re.sub(r'\\b'+key+r'\\b',value,doc)\n",
    "\n",
    "    # Mapping punctuations\n",
    "    for punct in punct_mapping:                                ## Normalization\n",
    "        if punct in doc:\n",
    "            doc = doc.replace(punct, punct_mapping[punct])\n",
    "\n",
    "    token = split_pattern.split(doc)                           ## Word tokenization\n",
    "\n",
    "    ## Removing leading and trailing special characters        ## Cleaning\n",
    "    token = [str1.strip(puncts) for str1 in token]\n",
    "    token = [str1.lstrip(puncts) for str1 in token]\n",
    "\n",
    "    ## Removing stop words\n",
    "    token = [i for i in token if i not in stopwords]           ## Cleaning - Stop words\n",
    "\n",
    "    ## Converting word to it's lemma\n",
    "    token = [lemma.lemmatize(word) for word in token]          ## Lemmatization\n",
    "    \n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 5572/5572 [00:03<00:00, 1437.31it/s]\n"
     ]
    }
   ],
   "source": [
    "fullCorpus['body_text_cleaned'] = fullCorpus['body_text'].progress_apply(lambda x: \" \".join(clean_text(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Feature Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right feature is secret sauce for better ML model which influence the results. Using domain knowledge of the data to create more features is feature engineering. Will consider character length of email and count number of punctuation used in email as a feature. Check below pinned article for more information. Check this __[article](https://medium.com/mindorks/what-is-feature-engineering-for-machine-learning-d8ba3158d97a)__ to understand feature engineering for ML and kaggle __[kernel](https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering)__ on Extensive feature engineering for text data.\n",
    "> #### Feature engineering is an art."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>body_text</th>\n",
       "      <th>body_text_cleaned</th>\n",
       "      <th>body_len</th>\n",
       "      <th>punct%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "      <td>go jurong point crazy available bugis n great world la e buffet cine got amore wat</td>\n",
       "      <td>92</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joking wif u oni</td>\n",
       "      <td>24</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry question(...</td>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "      <td>39</td>\n",
       "      <td>15.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>nah think go usf life around though</td>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                             body_text  \\\n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...   \n",
       "1                                                                        Ok lar... Joking wif u oni...   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "3                                                    U dun say so early hor... U c already then say...   \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "\n",
       "                                                                                     body_text_cleaned  \\\n",
       "0                   go jurong point crazy available bugis n great world la e buffet cine got amore wat   \n",
       "1                                                                              ok lar joking wif u oni   \n",
       "2  free entry 2 wkly comp win fa cup final tkts 21st may 2005 text fa 87121 receive entry question(...   \n",
       "3                                                                  u dun say early hor u c already say   \n",
       "4                                                                  nah think go usf life around though   \n",
       "\n",
       "   body_len  punct%  \n",
       "0        92     9.8  \n",
       "1        24    25.0  \n",
       "2       128     4.7  \n",
       "3        39    15.4  \n",
       "4        49     4.1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Create feature for text message length\n",
    "fullCorpus['body_len'] = fullCorpus['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "\n",
    "### Create feature for % of text that is punctuation\n",
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "fullCorpus['punct%'] = fullCorpus['body_text'].apply(lambda x: count_punct(x))\n",
    "\n",
    "fullCorpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moderation is the key to <s>success</s> great Machine learning model. Even though we can come up with more feature but only  relevant feature will really help the model in the learning process. Hence evaluate your feature set and keep only those which shows best result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally spam mails are longer in length and might have very frequent use of punctuation or upper cased words. Let's find out by plotting new feature as a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6571: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "pyplot.hist(fullCorpus[fullCorpus['label']=='spam']['body_len'], bins, alpha=0.5, normed=True, label='spam')\n",
    "pyplot.hist(fullCorpus[fullCorpus['label']=='ham']['body_len'], bins, alpha=0.5, normed=True, label='ham')\n",
    "pyplot.legend(loc='upper left')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi40LCBodHRwOi8vbWF0cGxvdGxpYi5vcmcv7US4rQAAGI1JREFUeJzt3X+QVeWd5/H3hx+CJlET7FhKQ7otsYomHXVsG61RN5iENBuFqQgbyFgDGyvsZiQ70TEKqV01WKmMyeyQqZJKhQQ3xB8B1+iErL0h/oqbmkKl8Rd0WGNLWLghpQSJozEoDd/9454m15uGe7r7dl+4z+dVRXHPc55z7vfR6+cezzn3OYoIzMwsDaNqXYCZmY0ch76ZWUIc+mZmCXHom5klxKFvZpYQh76ZWUIc+mZmCXHom5klxKFvZpaQMbUuoNxpp50WTU1NtS7DzOy4snnz5t9FREOlfsdc6Dc1NdHV1VXrMszMjiuS/l+efj69Y2aWEIe+mVlCHPpmZgk55s7pm5nlceDAAQqFAvv37691KSNq/PjxNDY2Mnbs2EFt79A3s+NSoVDgfe97H01NTUiqdTkjIiLYu3cvhUKB5ubmQe0j1+kdSR2SXpTUI2lpP+svk/SMpF5Jc8vWTZb0M0nbJP1SUtOgKjUzK7F//34mTJiQTOADSGLChAlD+r+biqEvaTSwEpgFtAALJLWUddsJLALu7WcXPwC+GRFTgXbg1UFXa2ZWIqXA7zPUMec5vdMO9ETE9uwN1wJzgF/2dYiIHdm6Q2XFtQBjIuLhrN+bQ6rWzMyGJE/oTwR2lSwXgOk5938O8HtJDwDNwCPA0og4WNpJ0mJgMcDkyZNz7trM7E9WPPyrqu7vuk+cU9X9HSvyhH5//y+R92nqY4BLgfMpngJaR/E00Op37SxiFbAKoK2trWZPaq/0oanXD4GZpSPPhdwCMKlkuRHYnXP/BeDZiNgeEb3AvwB/MbASzcyOTX/4wx/41Kc+xbnnnsuHP/xh1q1bR1NTEzfddBPt7e20t7fT09MDwE9+8hOmT5/O+eefz8c//nFeeeUVAG699VYWLlzIzJkzaWpq4oEHHuDGG2+ktbWVjo4ODhw4UNWa84T+JmCKpGZJJwDzgfU5978JeL+kvkmALqfkWoCZ2fHspz/9KWeeeSbPP/88W7dupaOjA4CTTz6Zp59+miVLlvClL30JgEsuuYQnn3ySZ599lvnz5/ONb3zj8H5efvllHnroIX784x9z9dVXM2PGDLZs2cKJJ57IQw89VNWaK4Z+doS+BNgAbAPui4huScslzQaQdKGkAjAP+I6k7mzbg8ANwKOStlA8VfTdqo7AzKxGWltbeeSRR7jpppv4xS9+wSmnnALAggULDv+9ceNGoPi7gk9+8pO0trbyzW9+k+7u7sP7mTVrFmPHjqW1tZWDBw8e/vJobW1lx44dVa0514+zIqIT6Cxru7nk9SaKp3362/Zh4CNDqNHM7Jh0zjnnsHnzZjo7O1m2bBkzZ84E3n1bZd/rL37xi1x//fXMnj2bn//859x6662H+4wbNw6AUaNGMXbs2MPbjBo1it7e3qrW7Ll3zMwGaffu3Zx00klcffXV3HDDDTzzzDMArFu37vDfF198MQCvv/46EydOBGDNmjW1KRhPw2BmdaIWd9dt2bKFL3/5y4eP0L/97W8zd+5c3n77baZPn86hQ4f44Q9/CBQv2M6bN4+JEydy0UUX8etf/3rE6wVQRM3ukOxXW1tb1OohKr5l0+z4sW3bNqZOnVrrMv5M34OgTjvttGF7j/7GLmlzRLRV2tand8zMEuLTO2ZmVVTtu22qzUf6ZmYJceibmSXEoW9mlhCHvplZQnwh18zqw+Nfr+7+Ziyr2GXHjh1cccUVbN26tbrvPYx8pG9mlhCHvpnZEBw8eJDPf/7zTJs2jZkzZ/LHP/6R7373u1x44YWce+65XHXVVbz11lsALFq0iC984QvMmDGDs846iyeeeILPfe5zTJ06lUWLFo1IvQ59M7MheOmll7j22mvp7u7m1FNP5Uc/+hGf/vSn2bRpE88//zxTp05l9eo/PTdq3759PPbYY6xYsYIrr7yS6667ju7ubrZs2cJzzz037PU69M3MhqC5uZnzzjsPgAsuuIAdO3awdetWLr30UlpbW7nnnnveNY3ylVdeiSRaW1s5/fTTaW1tZdSoUUybNm1Eftjl0DczG4K+aZEBRo8eTW9vL4sWLeKOO+5gy5Yt3HLLLezfv//P+o8aNepd2w7HNMr9ceibmVXZG2+8wRlnnMGBAwe45557al3Ou/iWTTOrDzlusRwpt912G9OnT+dDH/oQra2tvPHGG7Uu6bBcUytL6gD+GRgNfC8i/qFs/WXAtyg+IWt+RNxftv5kio9afDAilhztvTy1spnlcaxOrTwShnVqZUmjgZXALKAFWCCppazbTmARcO8RdnMb8ESl9zIzs+GV55x+O9ATEdsj4h1gLTCntENE7IiIF4BD5RtLugA4HfhZFeo1M7MhyBP6E4FdJcuFrK0iSaOA/w58uUK/xZK6JHXt2bMnz67NzDjWnvw3EoY65jyhr37a8r7r3wKdEbHraJ0iYlVEtEVEW0NDQ85dm1nKxo8fz969e5MK/ohg7969jB8/ftD7yHP3TgGYVLLcCOzOuf+LgUsl/S3wXuAESW9GxNKBlWlm9m6NjY0UCgVSOzswfvx4GhsbB719ntDfBEyR1Az8BpgPfDbPziPir/teS1oEtDnwzawaxo4dS3Nzc63LOO5UPL0TEb3AEmADxdsu74uIbknLJc0GkHShpAIwD/iOpO4j79HMzGol14+zIqIT6Cxru7nk9SaKp32Oto/vA98fcIVmZlY1nobBzCwhnoahivyLXjM71vlI38wsIQ59M7OEOPTNzBLi0DczS4hD38wsIQ59M7OEOPTNzBLi0DczS4hD38wsIQ59M7OEOPTNzBLi0DczS4hD38wsIUnNsllpFkwzs3qXVOgPlb80zOx4l+v0jqQOSS9K6pH0Z8+4lXSZpGck9UqaW9J+nqSNkrolvSDpM9Us3szMBqZi6EsaDawEZgEtwAJJLWXddgKLgHvL2t8C/iYipgEdwLcknTrUos3MbHDynN5pB3oiYjuApLXAHOCXfR0iYke27lDphhHxq5LXuyW9CjQAvx9y5WZmNmB5Tu9MBHaVLBeytgGR1A6cALzcz7rFkrokde3Zs2eguzYzs5zyhL76aYuBvImkM4C7gP8YEYfK10fEqohoi4i2hoaGgezazMwGIE/oF4BJJcuNwO68byDpZOAh4L9GxJMDK8/MzKopT+hvAqZIapZ0AjAfWJ9n51n/B4EfRMT/HHyZZmZWDRVDPyJ6gSXABmAbcF9EdEtaLmk2gKQLJRWAecB3JHVnm/8H4DJgkaTnsj/nDctIzMysolw/zoqITqCzrO3mktebKJ72Kd/ubuDuIdZoZmZV4rl3zMwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwS4tA3M0uIQ9/MLCF+MPoAXLRz1VHXPzl58QhVYmY2OD7SNzNLiEPfzCwhDn0zs4Q49M3MEuILuSUqXag1Mzve5TrSl9Qh6UVJPZKW9rP+MknPSOqVNLds3UJJL2V/FlarcDMzG7iKoS9pNLASmAW0AAsktZR12wksAu4t2/YDwC3AdKAduEXS+4detpmZDUaeI/12oCcitkfEO8BaYE5ph4jYEREvAIfKtv0k8HBEvBYR+4CHgY4q1G1mZoOQJ/QnArtKlgtZWx65tpW0WFKXpK49e/bk3LWZmQ1UntBXP22Rc/+5to2IVRHRFhFtDQ0NOXdtZmYDlSf0C8CkkuVGYHfO/Q9lWzMzq7I8ob8JmCKpWdIJwHxgfc79bwBmSnp/dgF3ZtZmZmY1UDH0I6IXWEIxrLcB90VEt6TlkmYDSLpQUgGYB3xHUne27WvAbRS/ODYBy7M2MzOrgVw/zoqITqCzrO3mktebKJ666W/bO4E7h1CjmZlViadhMDNLiEPfzCwhDn0zs4Q49M3MEuLQNzNLiEPfzCwhDn0zs4T4ISojaMXDvzriuus+cc4IVmJmqfKRvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQnz3ThVdtHPVUdc/OXnxCFViZtY/H+mbmSXEoW9mlpBcoS+pQ9KLknokLe1n/ThJ67L1T0lqytrHSlojaYukbZKWVbd8MzMbiIqhL2k0sBKYBbQACyS1lHW7BtgXEWcDK4Dbs/Z5wLiIaAUuAP5T3xeCmZmNvDxH+u1AT0Rsj4h3gLXAnLI+c4A12ev7gY9JEhDAeySNAU4E3gH+rSqVm5nZgOUJ/YnArpLlQtbWb5/sQeqvAxMofgH8AfgtsBP4Rz8Y3cysdvKEvvppi5x92oGDwJlAM/D3ks76szeQFkvqktS1Z8+eHCWZmdlg5An9AjCpZLkR2H2kPtmpnFOA14DPAj+NiAMR8Srwr0Bb+RtExKqIaIuItoaGhoGPwszMcsnz46xNwBRJzcBvgPkUw7zUemAhsBGYCzwWESFpJ3C5pLuBk4CLgG9Vq/h+Pf71o6y8aljf2szsWFfxSD87R78E2ABsA+6LiG5JyyXNzrqtBiZI6gGuB/pu61wJvBfYSvHL439ExAtVHoOZmeWUaxqGiOgEOsvabi55vZ/i7Znl273ZX7uZmdWGf5FrZpYQh76ZWUIc+mZmCXHom5klxKFvZpYQh76ZWUIc+mZmCXHom5klxKFvZpYQh76ZWUIc+mZmCXHom5klJNeEa1YdF+1cdZS1/zhidZhZunykb2aWEIe+mVlCHPpmZglx6JuZJSRX6EvqkPSipB5JS/tZP07Sumz9U5KaStZ9RNJGSd2StkgaX73yzcxsICqGvqTRFJ91OwtoARZIainrdg2wLyLOBlYAt2fbjgHuBv5zREwDPgocqFr1ZmY2IHmO9NuBnojYHhHvAGuBOWV95gBrstf3Ax+TJGAm8EJEPA8QEXsj4mB1Sjczs4HKE/oTgV0ly4Wsrd8+EdELvA5MAM4BQtIGSc9IunHoJZuZ2WDl+XGW+mmLnH3GAJcAFwJvAY9K2hwRj75rY2kxsBhg8uTJOUoyM7PByHOkXwAmlSw3AruP1Cc7j38K8FrW/kRE/C4i3gI6gb8of4OIWBURbRHR1tDQMPBRmJlZLnmO9DcBUyQ1A78B5gOfLeuzHlgIbATmAo9FREjaANwo6STgHeDfUbzQa+Ue//rR189YNjJ1mFldqxj6EdEraQmwARgN3BkR3ZKWA10RsR5YDdwlqYfiEf78bNt9kv6J4hdHAJ0R8dAwjcXMzCrINeFaRHRSPDVT2nZzyev9wLwjbHs3xds2zcysxvyLXDOzhDj0zcwS4tA3M0uIQ9/MLCEOfTOzhDj0zcwSktQzco/+jFozs/rnI30zs4Q49M3MEuLQNzNLiEPfzCwhDn0zs4Q49M3MEpLULZvHso3b9x51/cV4vn0zGzof6ZuZJcRH+vXCT94ysxx8pG9mlpBcoS+pQ9KLknokLe1n/ThJ67L1T0lqKls/WdKbkm6oTtlmZjYYFUNf0mhgJTALaAEWSGop63YNsC8izqb44PPby9avAP730Ms1M7OhyHOk3w70RMT2iHgHWAvMKeszB1iTvb4f+JgkAUj6K2A70F2dks3MbLDyhP5EYFfJciFr67dPRPQCrwMTJL0HuAn46tBLNTOzocoT+uqnLXL2+SqwIiLePOobSIsldUnq2rNnT46SzMxsMPLcslkAJpUsNwK7j9CnIGkMcArwGjAdmCvpG8CpwCFJ+yPijtKNI2IVsAqgra2t/AvFzMyqJE/obwKmSGoGfgPMBz5b1mc9sBDYCMwFHouIAC7t6yDpVuDN8sCvtkq/bDUzS1nF0I+IXklLgA3AaODOiOiWtBzoioj1wGrgLkk9FI/w5w9n0SmqOE3DWRNGqBIzO57l+kVuRHQCnWVtN5e83g/Mq7CPWwdRn5mZVZF/kWtmlhCHvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQhz6ZmYJceibmSXEoW9mlhCHvplZQnJNrWx14PGvH339jGUjU4eZ1ZSP9M3MEuLQNzNLSK7Ql9Qh6UVJPZKW9rN+nKR12fqnJDVl7Z+QtFnSluzvy6tbvpmZDUTF0Jc0GlgJzAJagAWSWsq6XQPsi4izgRXA7Vn774ArI6KV4oPT76pW4WZmNnB5jvTbgZ6I2B4R7wBrgTllfeYAa7LX9wMfk6SIeDYidmft3cB4SeOqUbiZmQ1cnrt3JgK7SpYLwPQj9YmIXkmvAxMoHun3uQp4NiLeHny5Nmx8d49ZEvKEvvppi4H0kTSN4imfmf2+gbQYWAwwefLkHCWZmdlg5Dm9UwAmlSw3AruP1EfSGOAU4LVsuRF4EPibiHi5vzeIiFUR0RYRbQ0NDQMbgZmZ5ZYn9DcBUyQ1SzoBmA+sL+uznuKFWoC5wGMREZJOBR4ClkXEv1araDMzG5yKoR8RvcASYAOwDbgvIrolLZc0O+u2GpggqQe4Hui7rXMJcDbw3yQ9l/35YNVHYWZmuSii/PR8bbW1tUVXV9egt9+4+oYqVnP8uPisCbV7c1/kNas5SZsjoq1SP/8i18wsIZ5wzYafbwc1O2Y49G3oKoW6mR0zfHrHzCwhDn0zs4Q49M3MEuJz+nVi4/a9R11f01s6zeyY4SN9M7OEOPTNzBLi0DczS4hD38wsIb6Qa7XnX+yajRgf6ZuZJcShb2aWEJ/eScRxfR//cM7t41NHlhiHvgHH+ZeCmeXm0Le0+SKyJSZX6EvqAP4ZGA18LyL+oWz9OOAHwAXAXuAzEbEjW7cMuAY4CPyXiNhQterNhpu/FKzOVAx9SaOBlcAngAKwSdL6iPhlSbdrgH0Rcbak+cDtwGcktVB8kPo04EzgEUnnRMTBag/ErCaGcr2h0hfGUL9w/IVl/chzpN8O9ETEdgBJa4E5QGnozwFuzV7fD9whSVn72oh4G/h19uD0dmBjdcq3kVLpnP9QVLpeMJT3rutrEcfyw2v8hXPMyhP6E4FdJcsFYPqR+kREr6TXgQlZ+5Nl204cdLVm9cShPfj3H4pafuHU+p8r+UJf/bRFzj55tkXSYmBxtvimpBdz1HUkpwG/G8L2x6PUxpzaeGFYxvyVGm2be/th+vc81NqH01eGMuYP5emUJ/QLwKSS5UZg9xH6FCSNAU4BXsu5LRGxCliVp+BKJHVFRFs19nW8SG3MqY0XPOZUjMSY8/widxMwRVKzpBMoXphdX9ZnPbAwez0XeCwiImufL2mcpGZgCvB0dUo3M7OBqnikn52jXwJsoHjL5p0R0S1pOdAVEeuB1cBd2YXa1yh+MZD1u4/iRd9e4FrfuWNmVju57tOPiE6gs6zt5pLX+4F5R9j2a8DXhlDjQFXlNNFxJrUxpzZe8JhTMexjVvEsjJmZpcCzbJqZJaRuQl9Sh6QXJfVIWlrreoaDpDslvSppa0nbByQ9LOml7O/317LGapM0SdLjkrZJ6pb0d1l73Y5b0nhJT0t6PhvzV7P2ZklPZWNel91YUTckjZb0rKT/lS3X9XgBJO2QtEXSc5K6srZh/WzXReiXTBUxC2gBFmRTQNSb7wMdZW1LgUcjYgrwaLZcT3qBv4+IqcBFwLXZv9t6HvfbwOURcS5wHtAh6SKK05usyMa8j+L0J/Xk74BtJcv1Pt4+MyLivJJbNYf1s10XoU/JVBER8Q7QN1VEXYmI/0Px7qhSc4A12es1wF+NaFHDLCJ+GxHPZK/foBgKE6njcUfRm9ni2OxPAJdTnOYE6mzMkhqBTwHfy5ZFHY+3gmH9bNdL6Pc3VUQq0z2cHhG/hWJAAh+scT3DRlITcD7wFHU+7uxUx3PAq8DDwMvA7yOiN+tSb5/xbwE3Aoey5QnU93j7BPAzSZuzmQlgmD/b9TKffq7pHuz4Jem9wI+AL0XEvxUPBOtX9nuW8ySdCjwITO2v28hWNTwkXQG8GhGbJX20r7mfrnUx3jJ/GRG7JX0QeFjS/x3uN6yXI/1c0z3UqVcknQGQ/f1qjeupOkljKQb+PRHxQNZc9+MGiIjfAz+neD3j1GyaE6ivz/hfArMl7aB4avZyikf+9TrewyJid/b3qxS/3NsZ5s92vYR+nqki6lXpFBgLgR/XsJaqy87trga2RcQ/layq23FLasiO8JF0IvBxitcyHqc4zQnU0ZgjYllENEZEE8X/dh+LiL+mTsfbR9J7JL2v7zUwE9jKMH+26+bHWZL+PcWjg76pIkbyV8AjQtIPgY9SnH3wFeAW4F+A+4DJwE5gXkSUX+w9bkm6BPgFsIU/ne/9CsXz+nU5bkkfoXgBbzTFA7P7ImK5pLMoHgl/AHgWuDp7VkXdyE7v3BARV9T7eLPxPZgtjgHujYivSZrAMH626yb0zcyssno5vWNmZjk49M3MEuLQNzNLiEPfzCwhDn0zs4Q49M3MEuLQNzNLiEPfzCwh/x/S5YJ9kgdfvgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "pyplot.hist(fullCorpus[fullCorpus['label']=='spam']['punct%'], bins, alpha=0.5, normed=True, label='spam')\n",
    "pyplot.hist(fullCorpus[fullCorpus['label']=='ham']['punct%'], bins, alpha=0.5, normed=True, label='ham')\n",
    "pyplot.legend(loc='upper right')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As thought spam emails are clearly longer in length. However, our hypothesis that spam emails contain more punctuation doesn't appear to be accurate, and it isn't quite clear whether this feature will provide value to the model. Now, in cases like this where there is some separation between the distributions, typically we'll err on the side of leaving this feature in the model just to see what kind of value the model itself may be able to extract out of it. So this is an example of how you might evaluate whether some newly created features will be useful to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into train/test\n",
    "\n",
    "Training dataset is the subset of dataset to train the model and test dataset is the subset to test the trained model.\n",
    "\n",
    "Make sure that test set is large enough to yield statistically meaningful results and don't pick a test set with different characteristics than the training set. Assuming that your test set meets the preceding two conditions, your goal is to create a model that generalizes well to new data. Check __[this](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6)__ article on spliting data into train and test and why it's required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punct% is dropped.\n",
    "def split_train_test(X,y,test_size=0.2) :\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text to Features- Vector representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning algorithm can only understand  numeric feature space hence we need to convert input text into a meaningful vector (or array) of numbers. This process is sometime referred as \"embedding\" or \"vectorization\". <br>\n",
    "There are various methods to do the same: Bag of words, TF-IDF, Word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words(Count Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bag-of-words model is a simplifying representation used in NLP. In this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = CountVectorizer(ngram_range=(1,1))\n",
    "count_vect = count.fit_transform(fullCorpus['body_text_cleaned'])\n",
    "\n",
    "X_count_feature = pd.concat([fullCorpus[['body_len']].reset_index(drop=True), pd.DataFrame(count_vect.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-idf stands for term frequency-inverse document frequency, and the tf-idf weight is a weight often used as features in ML algorithms. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(analyzer=clean_text)\n",
    "tfidf_vect = tfidf.fit_transform(fullCorpus['body_text_cleaned'])\n",
    "\n",
    "X_tfidf_feature = pd.concat([fullCorpus[['body_len']].reset_index(drop=True), pd.DataFrame(tfidf_vect.toarray())], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many moving parts in a Machine Learning (ML) model that have to be tied together for an ML model to execute and produce results successfully. This process of tying together different pieces of the ML process is known as a pipeline.<br>\n",
    "https://dzone.com/articles/how-to-build-a-simple-machine-learning-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, X_feature, y_feature, test_size):\n",
    "    X_train, X_test, y_train, y_test = split_train_test(X_feature, y_feature, test_size)\n",
    "    \n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(X_test)\n",
    "\n",
    "    return metrics.accuracy_score(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest model\n",
    "Fortunately, with libraries such as __[Scikit-Learn](https://scikit-learn.org/stable/)__, it’s now easy to implement __[hundreds of machine learning algorithms](https://scikit-learn.org/stable/supervised_learning.html)__ in Python. It’s so easy that we often don’t need any underlying knowledge of how the model works in order to use it. While knowing all the details is not necessary, it’s still helpful to have an idea of how a machine learning model works under the hood. This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models. Check this simple __[explanation](https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d)__ of random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier using count vectorizer as feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9748878923766816"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random Forest Classifier using count vectorizer as feature\")\n",
    "train_model(classifier=RandomForestClassifier(n_jobs=-1), \n",
    "            X_feature=X_count_feature, \n",
    "            y_feature=fullCorpus['label'],\n",
    "            test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier using TF-IDF as feature\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9748878923766816"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random Forest Classifier using TF-IDF as feature\")\n",
    "train_model(classifier=RandomForestClassifier(n_jobs=-1), \n",
    "            X_feature=X_tfidf_feature, \n",
    "            y_feature=fullCorpus['label'],\n",
    "            test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross - validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance. Find more about cross validation __[here](https://towardsdatascience.com/cross-validation-in-machine-learning-72924a69872f)__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation (classifier, X_feature, y_feature, n_splits=5) :\n",
    "    k_fold = KFold(n_splits=n_splits)\n",
    "    return cross_val_score(classifier, X_feature, y_feature, cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.97130045, 0.97578475, 0.97396768, 0.96229803, 0.97755835])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random Forest Classifier\")\n",
    "cross_validation(classifier=RandomForestClassifier(n_jobs=-1), \n",
    "            X_feature=X_count_feature,   \n",
    "            y_feature=fullCorpus['label'],\n",
    "            n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Classifier\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.96950673, 0.97309417, 0.96947935, 0.95601436, 0.96319569])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Random Forest Classifier\")\n",
    "cross_validation(classifier=RandomForestClassifier(n_jobs=-1), \n",
    "            X_feature=X_tfidf_feature,\n",
    "            y_feature=fullCorpus['label'],\n",
    "            n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross validation results seems good. 5 fold cross validation accuracy is not varying much which is good sign. It means model is generalizing well from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML/DL hyperparamater tuning is like setting right frequncy of an radio channel to listen noise free audio. Similary grid-search is a method to find the best combination of hyper-parameters (an example of an hyper-parameter is the learning rate of the optimiser), for a given model (e.g. a CNN) and test dataset. Check __[this](https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568)__ article on understanding hyperparameters and it's optimisation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split3_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('split4_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "c:\\users\\yogesh.k\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\deprecation.py:125: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>44.049026</td>\n",
       "      <td>0.960341</td>\n",
       "      <td>0.352845</td>\n",
       "      <td>0.021502</td>\n",
       "      <td>None</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 300}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.977558</td>\n",
       "      <td>...</td>\n",
       "      <td>0.977207</td>\n",
       "      <td>0.002383</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>22.543966</td>\n",
       "      <td>0.567374</td>\n",
       "      <td>0.213814</td>\n",
       "      <td>0.005129</td>\n",
       "      <td>None</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 150}</td>\n",
       "      <td>0.981166</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.976661</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976669</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>2</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38.558354</td>\n",
       "      <td>0.454792</td>\n",
       "      <td>0.342384</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>90</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 300}</td>\n",
       "      <td>0.979372</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.976131</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>3</td>\n",
       "      <td>0.999551</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999282</td>\n",
       "      <td>0.000168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20.690266</td>\n",
       "      <td>1.028941</td>\n",
       "      <td>0.184013</td>\n",
       "      <td>0.011682</td>\n",
       "      <td>90</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 150}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975772</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>4</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999327</td>\n",
       "      <td>0.999103</td>\n",
       "      <td>0.999192</td>\n",
       "      <td>0.000110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.123855</td>\n",
       "      <td>1.057890</td>\n",
       "      <td>0.156797</td>\n",
       "      <td>0.020202</td>\n",
       "      <td>60</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 150}</td>\n",
       "      <td>0.978475</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.972172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.975233</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>5</td>\n",
       "      <td>0.994166</td>\n",
       "      <td>0.993045</td>\n",
       "      <td>0.994841</td>\n",
       "      <td>0.996411</td>\n",
       "      <td>0.994168</td>\n",
       "      <td>0.994526</td>\n",
       "      <td>0.001105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.080525</td>\n",
       "      <td>0.086314</td>\n",
       "      <td>0.067458</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 90, 'n_estimators': 10}</td>\n",
       "      <td>0.982063</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.974865</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974874</td>\n",
       "      <td>0.004135</td>\n",
       "      <td>6</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.998205</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>0.996635</td>\n",
       "      <td>0.997936</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.775573</td>\n",
       "      <td>0.027399</td>\n",
       "      <td>0.068802</td>\n",
       "      <td>0.001491</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 10}</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.973968</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974336</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>7</td>\n",
       "      <td>0.992596</td>\n",
       "      <td>0.993269</td>\n",
       "      <td>0.995289</td>\n",
       "      <td>0.991925</td>\n",
       "      <td>0.995514</td>\n",
       "      <td>0.993719</td>\n",
       "      <td>0.001440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>29.947295</td>\n",
       "      <td>1.452509</td>\n",
       "      <td>0.307038</td>\n",
       "      <td>0.013565</td>\n",
       "      <td>60</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 60, 'n_estimators': 300}</td>\n",
       "      <td>0.976682</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.973070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.973798</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>8</td>\n",
       "      <td>0.994840</td>\n",
       "      <td>0.993942</td>\n",
       "      <td>0.995514</td>\n",
       "      <td>0.994616</td>\n",
       "      <td>0.994616</td>\n",
       "      <td>0.994706</td>\n",
       "      <td>0.000504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.211782</td>\n",
       "      <td>0.084800</td>\n",
       "      <td>0.071574</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': None, 'n_estimators': 10}</td>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.973094</td>\n",
       "      <td>0.971275</td>\n",
       "      <td>...</td>\n",
       "      <td>0.970747</td>\n",
       "      <td>0.002709</td>\n",
       "      <td>9</td>\n",
       "      <td>0.996410</td>\n",
       "      <td>0.996410</td>\n",
       "      <td>0.997533</td>\n",
       "      <td>0.996635</td>\n",
       "      <td>0.996411</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>0.000435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.788071</td>\n",
       "      <td>0.247401</td>\n",
       "      <td>0.133041</td>\n",
       "      <td>0.008404</td>\n",
       "      <td>30</td>\n",
       "      <td>150</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 150}</td>\n",
       "      <td>0.971300</td>\n",
       "      <td>0.968610</td>\n",
       "      <td>0.964093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965901</td>\n",
       "      <td>0.003985</td>\n",
       "      <td>10</td>\n",
       "      <td>0.977788</td>\n",
       "      <td>0.977563</td>\n",
       "      <td>0.977120</td>\n",
       "      <td>0.978241</td>\n",
       "      <td>0.976671</td>\n",
       "      <td>0.977477</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.933928</td>\n",
       "      <td>0.166641</td>\n",
       "      <td>0.192241</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n",
       "      <td>0.969507</td>\n",
       "      <td>0.967713</td>\n",
       "      <td>0.964093</td>\n",
       "      <td>...</td>\n",
       "      <td>0.965363</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>11</td>\n",
       "      <td>0.977339</td>\n",
       "      <td>0.976217</td>\n",
       "      <td>0.977793</td>\n",
       "      <td>0.977568</td>\n",
       "      <td>0.976895</td>\n",
       "      <td>0.977163</td>\n",
       "      <td>0.000558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.157148</td>\n",
       "      <td>0.040893</td>\n",
       "      <td>0.062865</td>\n",
       "      <td>0.009867</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 10}</td>\n",
       "      <td>0.975785</td>\n",
       "      <td>0.961435</td>\n",
       "      <td>0.962298</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963029</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>12</td>\n",
       "      <td>0.976890</td>\n",
       "      <td>0.973076</td>\n",
       "      <td>0.975998</td>\n",
       "      <td>0.974877</td>\n",
       "      <td>0.975550</td>\n",
       "      <td>0.975278</td>\n",
       "      <td>0.001281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "11      44.049026      0.960341         0.352845        0.021502   \n",
       "10      22.543966      0.567374         0.213814        0.005129   \n",
       "8       38.558354      0.454792         0.342384        0.010566   \n",
       "7       20.690266      1.028941         0.184013        0.011682   \n",
       "4       15.123855      1.057890         0.156797        0.020202   \n",
       "6        2.080525      0.086314         0.067458        0.002273   \n",
       "3        1.775573      0.027399         0.068802        0.001491   \n",
       "5       29.947295      1.452509         0.307038        0.013565   \n",
       "9        2.211782      0.084800         0.071574        0.004143   \n",
       "1        8.788071      0.247401         0.133041        0.008404   \n",
       "2       17.933928      0.166641         0.192241        0.006275   \n",
       "0        1.157148      0.040893         0.062865        0.009867   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "11            None                300   \n",
       "10            None                150   \n",
       "8               90                300   \n",
       "7               90                150   \n",
       "4               60                150   \n",
       "6               90                 10   \n",
       "3               60                 10   \n",
       "5               60                300   \n",
       "9             None                 10   \n",
       "1               30                150   \n",
       "2               30                300   \n",
       "0               30                 10   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "11  {'max_depth': None, 'n_estimators': 300}           0.978475   \n",
       "10  {'max_depth': None, 'n_estimators': 150}           0.981166   \n",
       "8     {'max_depth': 90, 'n_estimators': 300}           0.979372   \n",
       "7     {'max_depth': 90, 'n_estimators': 150}           0.978475   \n",
       "4     {'max_depth': 60, 'n_estimators': 150}           0.978475   \n",
       "6      {'max_depth': 90, 'n_estimators': 10}           0.982063   \n",
       "3      {'max_depth': 60, 'n_estimators': 10}           0.973991   \n",
       "5     {'max_depth': 60, 'n_estimators': 300}           0.976682   \n",
       "9    {'max_depth': None, 'n_estimators': 10}           0.973991   \n",
       "1     {'max_depth': 30, 'n_estimators': 150}           0.971300   \n",
       "2     {'max_depth': 30, 'n_estimators': 300}           0.969507   \n",
       "0      {'max_depth': 30, 'n_estimators': 10}           0.975785   \n",
       "\n",
       "    split1_test_score  split2_test_score       ...         mean_test_score  \\\n",
       "11           0.976682           0.977558       ...                0.977207   \n",
       "10           0.977578           0.976661       ...                0.976669   \n",
       "8            0.977578           0.974865       ...                0.976131   \n",
       "7            0.975785           0.973968       ...                0.975772   \n",
       "4            0.973991           0.972172       ...                0.975233   \n",
       "6            0.971300           0.974865       ...                0.974874   \n",
       "3            0.977578           0.973968       ...                0.974336   \n",
       "5            0.973991           0.973070       ...                0.973798   \n",
       "9            0.973094           0.971275       ...                0.970747   \n",
       "1            0.968610           0.964093       ...                0.965901   \n",
       "2            0.967713           0.964093       ...                0.965363   \n",
       "0            0.961435           0.962298       ...                0.963029   \n",
       "\n",
       "    std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "11        0.002383                1            1.000000            1.000000   \n",
       "10        0.003167                2            1.000000            1.000000   \n",
       "8         0.003247                3            0.999551            0.999103   \n",
       "7         0.001509                4            0.999327            0.999103   \n",
       "4         0.002385                5            0.994166            0.993045   \n",
       "6         0.004135                6            0.998205            0.998205   \n",
       "3         0.004839                7            0.992596            0.993269   \n",
       "5         0.003475                8            0.994840            0.993942   \n",
       "9         0.002709                9            0.996410            0.996410   \n",
       "1         0.003985               10            0.977788            0.977563   \n",
       "2         0.003758               11            0.977339            0.976217   \n",
       "0         0.007737               12            0.976890            0.973076   \n",
       "\n",
       "    split2_train_score  split3_train_score  split4_train_score  \\\n",
       "11            1.000000            1.000000            1.000000   \n",
       "10            1.000000            1.000000            1.000000   \n",
       "8             0.999327            0.999327            0.999103   \n",
       "7             0.999103            0.999327            0.999103   \n",
       "4             0.994841            0.996411            0.994168   \n",
       "6             0.998205            0.998430            0.996635   \n",
       "3             0.995289            0.991925            0.995514   \n",
       "5             0.995514            0.994616            0.994616   \n",
       "9             0.997533            0.996635            0.996411   \n",
       "1             0.977120            0.978241            0.976671   \n",
       "2             0.977793            0.977568            0.976895   \n",
       "0             0.975998            0.974877            0.975550   \n",
       "\n",
       "    mean_train_score  std_train_score  \n",
       "11          1.000000         0.000000  \n",
       "10          1.000000         0.000000  \n",
       "8           0.999282         0.000168  \n",
       "7           0.999192         0.000110  \n",
       "4           0.994526         0.001105  \n",
       "6           0.997936         0.000656  \n",
       "3           0.993719         0.001440  \n",
       "5           0.994706         0.000504  \n",
       "9           0.996680         0.000435  \n",
       "1           0.977477         0.000541  \n",
       "2           0.977163         0.000558  \n",
       "0           0.975278         0.001281  \n",
       "\n",
       "[12 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "param = {'n_estimators': [10, 150, 300],\n",
    "        'max_depth': [30, 60, 90, None]}\n",
    "\n",
    "gs = GridSearchCV(rf, param, cv=5, n_jobs=-1)\n",
    "gs_fit = gs.fit(X_count_feature, fullCorpus['label'])            ## Also try with X_tfidf_feature\n",
    "pd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Mean values in above table is the average value for each set of parameters across all the folds. For instance if you have 10 folds in your CV then mean_score_time will be the average prediction time across the 10 folds. There is obviously some variation across each fold which is indicated by standard deviation(std) where you can use std_score_time to get an idea for how those values vary around that mean value. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above table is sorted based on **mean_test_score** which has mean training time(mean_fit_time) of ~23 sec which is very high campared to ~0.81 sec(row with index number 0) because of low complexity of model ({'max_depth': 30, 'n_estimators': 10}) but it also has least accuracy compared to others. <br><br>\n",
    "We might be tempted to choose the model with best accuracy but other parameter like training time, inference time(prediction or score time), memory usage etc also plays important role in deciding the final model.<br><br>\n",
    "We choose the model(index id 3) which has acceptable training and inference time without comprising accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric\n",
    "\n",
    "Evaluating your machine learning algorithm is an essential part of any project. There are various kind of metrics to evaluate our models. Check __[here](https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c)__ why accuracy is not the right evaluation metric. Check this __[article](https://medium.com/usf-msds/choosing-the-right-metric-for-evaluating-machine-learning-models-part-2-86d5649a5428)__ or __[video](https://www.coursera.org/lecture/machine-learning-with-python/evaluation-metrics-in-classification-5iCQt)__ on evaluation metrics in classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step\n",
    "\n",
    "Once we have decided on features, vectorization methods, hyperparameter values etc. Will put all pieces together and create final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 0.673 / Predict time: 0.177 ---- Precision: 0.973 / Recall: 0.862 / Accuracy: 0.976\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=10, max_depth=90, n_jobs=-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_train_test(X_count_feature, fullCorpus['label'], 0.2)\n",
    "\n",
    "start = time.time()\n",
    "rf_model = rf.fit(X_train, y_train)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = rf_model.predict(X_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==y_test).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Mostly this is the end to end pipeline for the any text classification problem. If you are reading this then I hope you are now confident enough attempt any text classification problem. You can try solving the same problem with any other Machine learning classifier like SVM, Logistic regression etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future scope:\n",
    "-  We will use word2vec for text to feature conversion.\n",
    "-  Solve complex text classification problem(from Kaggle) using deep learning in Keras. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **This tutorial is intended to be a public resource. As such, if you see any glaring inaccuracies or if a critical topic is missing, please feel free to point it out or (preferably) submit a pull request to improve the tutorial. Also, we are always looking to improve the scope of this article. For anything feel free to mail us @ yogesh.kothiya.101@gmail.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Author of this article is Yogesh Kothiya. You can follow him on __[LinkedIn](https://www.linkedin.com/in/yogeshkothiya/)__, __[Medium](https://medium.com/@kothiya.yogesh)__, __[GitHub](https://github.com/kothiyayogesh)__, __[Twitter](https://twitter.com/Yogesh_Kothiya)__.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
